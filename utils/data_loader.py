import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from utils.config import SAMPLE_DATA_PATH, PRODUCTION_DATA_PATH, EXCEL_DATA_PATH
import os
import re
from datetime import datetime

def load_data(source_type='sample', sheet_url=None, credentials_path='credentials.json', file_path=None):
    """
    Load real estate data from different sources
    
    Args:
        source_type: 'sample' | 'csv' | 'excel' | 'gsheet'
        sheet_url: Required for 'gsheet' type
        credentials_path: Path to Google Service Account credentials
        file_path: Optional custom file path for csv/excel
        
    Returns:
        pandas.DataFrame
    """
    if source_type == 'sample':
        return pd.read_csv(SAMPLE_DATA_PATH)
    
    elif source_type == 'csv':
        file_to_read = file_path or PRODUCTION_DATA_PATH
        return pd.read_csv(file_to_read)
    
    elif source_type == 'excel':
        file_to_read = file_path or EXCEL_DATA_PATH
        return load_excel_file(file_to_read)
    
    elif source_type == 'gsheet':
        return load_google_sheet(sheet_url, credentials_path)
    
    else:
        raise ValueError(f"Invalid source type: {source_type}")

def load_excel_file(file_path):
    """Load data from Excel file (.xls or .xlsx)"""
    try:
        # Try to read the Excel file
        df = pd.read_excel(file_path)
        
        # Basic validation
        if df.empty:
            raise ValueError("Excel file is empty")
        
        print(f"‚úÖ Successfully loaded Excel file: {file_path}")
        print(f"üìä File contains {len(df)} rows and {len(df.columns)} columns")
        print(f"üìã Columns: {list(df.columns)}")
        
        return df
        
    except ImportError as e:
        if "xlrd" in str(e):
            raise ImportError(
                "xlrd library not found. Install with: pip install xlrd"
            )
        elif "openpyxl" in str(e):
            raise ImportError(
                "openpyxl library not found. Install with: pip install openpyxl"
            )
        else:
            raise ImportError(f"Excel reading library not found: {e}")
    
    except Exception as e:
        raise Exception(f"Error reading Excel file {file_path}: {str(e)}")

def process_landsoft_data(df):
    """
    Process LandSoft Excel data to match expected format
    """
    print("üîÑ Processing LandSoft data...")
    
    # Create a copy for processing
    processed_df = df.copy()
    
    # Map LandSoft columns to expected format
    column_mapping = {
        'Gallery': 'id',
        'M√£ s·∫£n ph·∫©m': 'product_id',
        'Nhu c·∫ßu': 'transaction_type',
        'S·ªë nh√†': 'house_number',
        'Lo·∫°i ƒë∆∞·ªùng': 'street_type',
        'T√™n ƒë∆∞·ªùng': 'street_name',
        'X√£/Ph∆∞·ªùng': 'ward',
        'Qu·∫≠n/huy·ªán': 'district',
        'Ngang XD': 'width',
        'D√†i XD': 'length',
        'Di·ªán t√≠ch': 'area',
        'T·ªïng gi√° text': 'price_text',
        'H∆∞·ªõng': 'direction',
        'Ch·ªß nh√†': 'owner',
        'ƒêi·ªán tho·∫°i': 'phone',
        'Di·ªÖn gi·∫£i': 'description',
        'Ng√†y ƒêK': 'registration_date',
        'Ng√†y c·∫≠p nh·∫≠t': 'update_date',
        'T·ª∑ l·ªá MG': 'commission_rate',
        'CV m√¥i gi·ªõi': 'agent_name',
        'CV ƒëƒÉng tin': 'posted_by'
    }
    
    # Rename columns
    processed_df = processed_df.rename(columns=column_mapping)
    
    # Generate unique ID if not exists
    if 'id' not in processed_df.columns:
        processed_df['id'] = processed_df['product_id'].fillna('SP') + '_' + processed_df.index.astype(str)
    
    # Process address
    processed_df['address'] = processed_df.apply(
        lambda row: f"{row.get('house_number', '')} {row.get('street_name', '')}, {row.get('ward', '')}, {row.get('district', '')}".strip(),
        axis=1
    )
    
    # Process price
    processed_df['price'] = processed_df['price_text'].apply(parse_price_text)
    
    # Process property type based on transaction type and description
    processed_df['type'] = processed_df.apply(determine_property_type, axis=1)
    
    # Extract bedrooms from description
    processed_df['bedrooms'] = processed_df['description'].apply(extract_bedrooms)
    
    # Process legal status (default to available)
    processed_df['legal_status'] = 'S·ªï h·ªìng'  # Default value
    
    # Process amenities from description
    processed_df['amenities'] = processed_df['description'].apply(extract_amenities)
    
    # Process status based on transaction type
    processed_df['status'] = processed_df['transaction_type'].apply(
        lambda x: 'available' if x == 'C·∫ßn b√°n' else 'for_rent' if x == 'Cho thu√™' else 'available'
    )
    
    # Add posted_date
    processed_df['posted_date'] = processed_df['registration_date'].apply(
        lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) else datetime.now().strftime('%Y-%m-%d')
    )
    
    print(f"‚úÖ Processed {len(processed_df)} records")
    return processed_df

def parse_price_text(price_text):
    """
    Parse price text to numeric value
    Examples: "50 tri·ªáu" -> 50000000, "6 t·ª∑ 900 tri·ªáu" -> 6900000000
    """
    if pd.isna(price_text) or price_text == 'Th∆∞∆°ng l∆∞·ª£ng':
        return 0
    
    price_text = str(price_text).strip()
    
    # Handle "Th∆∞∆°ng l∆∞·ª£ng" or empty
    if 'th∆∞∆°ng l∆∞·ª£ng' in price_text.lower() or price_text == '':
        return 0
    
    total_price = 0
    
    # Extract billions (t·ª∑)
    billion_match = re.search(r'(\d+(?:\.\d+)?)\s*t·ª∑', price_text, re.IGNORECASE)
    if billion_match:
        total_price += float(billion_match.group(1)) * 1000000000
    
    # Extract millions (tri·ªáu)
    million_match = re.search(r'(\d+(?:\.\d+)?)\s*tri·ªáu', price_text, re.IGNORECASE)
    if million_match:
        total_price += float(million_match.group(1)) * 1000000
    
    # Extract thousands (ngh√¨n)
    thousand_match = re.search(r'(\d+(?:\.\d+)?)\s*ngh√¨n', price_text, re.IGNORECASE)
    if thousand_match:
        total_price += float(thousand_match.group(1)) * 1000
    
    return int(total_price) if total_price > 0 else 0

def determine_property_type(row):
    """
    Determine property type based on transaction type and description
    """
    description = str(row.get('description', '')).lower()
    transaction_type = str(row.get('transaction_type', '')).lower()
    
    # Check for specific keywords in description
    if any(word in description for word in ['cƒÉn h·ªô', 'apartment', 'chung c∆∞']):
        return 'CƒÉn h·ªô'
    elif any(word in description for word in ['nh√† ph·ªë', 'shophouse', 'nh√† m·∫∑t ti·ªÅn']):
        return 'Nh√† ph·ªë'
    elif any(word in description for word in ['bi·ªát th·ª±', 'villa']):
        return 'Bi·ªát th·ª±'
    elif any(word in description for word in ['vƒÉn ph√≤ng', 'office']):
        return 'VƒÉn ph√≤ng'
    elif any(word in description for word in ['ƒë·∫•t n·ªÅn', 'ƒë·∫•t th·ªï c∆∞']):
        return 'ƒê·∫•t n·ªÅn'
    else:
        # Default based on transaction type
        if 'cho thu√™' in transaction_type:
            return 'CƒÉn h·ªô'  # Most common for rental
        else:
            return 'Nh√† ph·ªë'  # Most common for sale

def extract_bedrooms(description):
    """
    Extract number of bedrooms from description
    """
    if pd.isna(description):
        return 0
    
    description = str(description).lower()
    
    # Look for bedroom patterns
    patterns = [
        r'(\d+)\s*ph√≤ng\s*ng·ªß',
        r'(\d+)\s*pn',
        r'(\d+)\s*bedroom',
        r'(\d+)\s*br'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, description)
        if match:
            return int(match.group(1))
    
    return 0

def extract_amenities(description):
    """
    Extract amenities from description
    """
    if pd.isna(description):
        return ''
    
    description = str(description).lower()
    amenities = []
    
    # Common amenities to look for
    amenity_keywords = {
        'h·ªì b∆°i': 'H·ªì b∆°i',
        'gym': 'Gym',
        'thang m√°y': 'Thang m√°y',
        'b√£i xe': 'B√£i xe',
        'an ninh': 'An ninh 24/7',
        's√¢n ch∆°i': 'S√¢n ch∆°i tr·∫ª em',
        'v∆∞·ªùn': 'V∆∞·ªùn',
        's√¢n th∆∞·ª£ng': 'S√¢n th∆∞·ª£ng',
        'ban c√¥ng': 'Ban c√¥ng',
        'nh√† b·∫øp': 'Nh√† b·∫øp',
        'ph√≤ng kh√°ch': 'Ph√≤ng kh√°ch',
        'wc': 'WC ri√™ng',
        'ƒëi·ªÅu h√≤a': 'ƒêi·ªÅu h√≤a',
        'n√≥ng l·∫°nh': 'N√≥ng l·∫°nh',
        'internet': 'Internet',
        'truy·ªÅn h√¨nh': 'Truy·ªÅn h√¨nh c√°p'
    }
    
    for keyword, amenity in amenity_keywords.items():
        if keyword in description:
            amenities.append(amenity)
    
    return ', '.join(amenities) if amenities else 'C∆° b·∫£n'

def load_google_sheet(sheet_url, credentials_path='credentials.json'):
    """Load data from Google Sheet using service account credentials"""
    # Set up authentication
    scopes = ['https://www.googleapis.com/auth/spreadsheets']
    
    if not os.path.exists(credentials_path):
        raise FileNotFoundError(
            f"Google credentials file not found at {credentials_path}. "
            "Please download from Google Cloud Console"
        )
    
    creds = Credentials.from_service_account_file(
        credentials_path,
        scopes=scopes
    )
    
    # Connect to Google Sheets
    client = gspread.authorize(creds)
    
    try:
        # Open the spreadsheet
        spreadsheet = client.open_by_url(sheet_url)
        
        # Get the first worksheet
        worksheet = spreadsheet.get_worksheet(0)
        
        # Get all records as a list of dictionaries
        records = worksheet.get_all_records()
        
        # Convert to DataFrame
        return pd.DataFrame(records)
        
    except Exception as e:
        raise ConnectionError(
            f"Failed to access Google Sheet: {str(e)}\n"
            "Ensure: \n"
            "1. Sheet URL is correct\n"
            "2. Service account has been granted access\n"
            "3. Sheet structure matches expected format"
        )

def analyze_data_structure(df, source_type):
    """Analyze and report data structure"""
    print(f"\nüìä Data Structure Analysis for {source_type}:")
    print("=" * 50)
    print(f"Total rows: {len(df)}")
    print(f"Total columns: {len(df.columns)}")
    print(f"Columns: {list(df.columns)}")
    
    # Check for required columns
    required_columns = ['id', 'type', 'district', 'ward', 'address', 'price', 'area', 'bedrooms', 'direction', 'legal_status', 'amenities', 'description']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        print(f"\n‚ö†Ô∏è Missing required columns: {missing_columns}")
        print("üí° You may need to map your Excel columns to the expected format")
    else:
        print(f"\n‚úÖ All required columns found!")
    
    return missing_columns