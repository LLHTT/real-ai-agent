import os
import pandas as pd
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from utils.config import *
from utils.data_loader import load_data, analyze_data_structure

# Load env
load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Load v√† x·ª≠ l√Ω d·ªØ li·ªáu
def load_and_process_data(source_type='sample', sheet_url=None, credentials_path=None, file_path=None):
    """
    Load and process data from different sources
    
    Args:
        source_type: 'sample', 'csv', 'excel', or 'gsheet'
        sheet_url: Required for 'gsheet' type
        credentials_path: Path to Google credentials (optional, uses default if None)
        file_path: Optional custom file path for csv/excel
    """
    try:
        # Load data from source
        if source_type == 'gsheet':
            if not sheet_url:
                raise ValueError("Sheet URL is required for Google Sheets source")
            creds_path = credentials_path or str(GOOGLE_CREDENTIALS_PATH)
            df = load_data(source_type='gsheet', sheet_url=sheet_url, credentials_path=creds_path)
        else:
            df = load_data(source_type=source_type, file_path=file_path)
        
        # Analyze data structure
        missing_columns = analyze_data_structure(df, source_type)
        
        # If missing required columns, try to map them
        if missing_columns:
            df = map_excel_columns(df)
        
        # Validate required columns after mapping
        required_columns = ['id', 'type', 'district', 'ward', 'address', 'price', 'area', 'bedrooms', 'direction', 'legal_status', 'amenities', 'description']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns after mapping: {missing_columns}")
        
        # T·∫°o text embedding cho m·ªói s·∫£n ph·∫©m
        df['text'] = df.apply(lambda row: f"""
        M√£ SP: {row['id']}
        Lo·∫°i h√¨nh: {row['type']}
        V·ªã tr√≠: {row['district']}, {row['ward']}
        ƒê·ªãa ch·ªâ: {row['address']}
        Gi√°: {row['price']:,.0f} VND
        Di·ªán t√≠ch: {row['area']}m¬≤
        Ph√≤ng ng·ªß: {row['bedrooms']}
        H∆∞·ªõng: {row['direction']}
        Ph√°p l√Ω: {row['legal_status']}
        Ti·ªán √≠ch: {row['amenities']}
        M√¥ t·∫£: {row['description']}
        """, axis=1)
        
        return df
        
    except Exception as e:
        raise Exception(f"Error loading data from {source_type}: {str(e)}")

def map_excel_columns(df):
    """Map Excel columns to expected format"""
    print("\nüîÑ Attempting to map Excel columns to expected format...")
    
    # Common column mappings
    column_mappings = {
        # ID mappings
        'm√£': 'id', 'm√£ sp': 'id', 'm√£ s·∫£n ph·∫©m': 'id', 'product_id': 'id', 'id': 'id',
        
        # Type mappings
        'lo·∫°i h√¨nh': 'type', 'lo·∫°i': 'type', 'property_type': 'type', 'type': 'type',
        
        # Location mappings
        'qu·∫≠n': 'district', 'district': 'district',
        'ph∆∞·ªùng': 'ward', 'ward': 'ward',
        'ƒë·ªãa ch·ªâ': 'address', 'address': 'address', 'ƒë·ªãa ch·ªâ ƒë·∫ßy ƒë·ªß': 'address',
        
        # Price mappings
        'gi√°': 'price', 'price': 'price', 'gi√° b√°n': 'price', 'sale_price': 'price',
        
        # Area mappings
        'di·ªán t√≠ch': 'area', 'area': 'area', 'di·ªán t√≠ch m2': 'area', 'square_meters': 'area',
        
        # Bedroom mappings
        'ph√≤ng ng·ªß': 'bedrooms', 'bedrooms': 'bedrooms', 's·ªë ph√≤ng ng·ªß': 'bedrooms',
        
        # Direction mappings
        'h∆∞·ªõng': 'direction', 'direction': 'direction', 'h∆∞·ªõng nh√†': 'direction',
        
        # Legal status mappings
        'ph√°p l√Ω': 'legal_status', 'legal_status': 'legal_status', 't√¨nh tr·∫°ng ph√°p l√Ω': 'legal_status',
        
        # Amenities mappings
        'ti·ªán √≠ch': 'amenities', 'amenities': 'amenities', 'facilities': 'amenities',
        
        # Description mappings
        'm√¥ t·∫£': 'description', 'description': 'description', 'chi ti·∫øt': 'description'
    }
    
    # Create a copy of the dataframe
    mapped_df = df.copy()
    
    # Try to map columns
    mapped_columns = {}
    for excel_col in df.columns:
        excel_col_lower = str(excel_col).lower().strip()
        
        # Check for exact matches first
        if excel_col_lower in column_mappings:
            mapped_columns[excel_col] = column_mappings[excel_col_lower]
            print(f"  ‚úÖ Mapped '{excel_col}' ‚Üí '{column_mappings[excel_col_lower]}'")
            continue
        
        # Check for partial matches
        for key, value in column_mappings.items():
            if key in excel_col_lower or excel_col_lower in key:
                mapped_columns[excel_col] = value
                print(f"  ‚úÖ Mapped '{excel_col}' ‚Üí '{value}' (partial match)")
                break
    
    # Rename columns
    if mapped_columns:
        mapped_df = mapped_df.rename(columns=mapped_columns)
        print(f"  üìä Successfully mapped {len(mapped_columns)} columns")
    else:
        print("  ‚ö†Ô∏è No columns could be automatically mapped")
    
    return mapped_df

# Kh·ªüi t·∫°o vector store
def init_vector_store(df, source_type='sample'):
    """
    Initialize vector store with data
    
    Args:
        df: Processed DataFrame
        source_type: Source type for cache management
    """
    try:
        texts = df['text'].tolist()
        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
        
        # Create unique collection name based on source type
        collection_name = f"real_estate_{source_type}"
        
        vector_store = Chroma.from_texts(
            texts=texts,
            embedding=embeddings,
            persist_directory=str(VECTOR_DB_DIR),
            collection_name=collection_name
        )
        
        return vector_store.as_retriever(search_kwargs={"k": TOP_K_RESULTS})
        
    except Exception as e:
        raise Exception(f"Error initializing vector store: {str(e)}")

# T·∫°o AI chain
def create_agent(source_type='sample', sheet_url=None, credentials_path=None, file_path=None):
    """
    Create AI agent with specified data source
    
    Args:
        source_type: Data source type ('sample', 'csv', 'excel', 'gsheet')
        sheet_url: Google Sheet URL (required for 'gsheet')
        credentials_path: Path to Google credentials (optional)
        file_path: Optional custom file path for csv/excel
    """
    try:
        # Load and process data
        df = load_and_process_data(source_type, sheet_url, credentials_path, file_path)
        
        # Initialize vector store
        retriever = init_vector_store(df, source_type)
        
        # Create prompt and LLM
        prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
        llm = ChatOpenAI(model=LLM_MODEL, temperature=LLM_TEMPERATURE)
        
        # Create chain
        chain = (
            {"context": retriever, "question": RunnablePassthrough()} 
            | prompt 
            | llm 
            | StrOutputParser()
        )
        
        return chain, df
        
    except Exception as e:
        raise Exception(f"Error creating agent: {str(e)}")

# Kh·ªüi t·∫°o default agent (for backward compatibility)
def get_default_agent():
    """Get default agent using sample data"""
    return create_agent(source_type='sample')

# Global agent instance (will be updated based on user selection)
real_estate_agent = get_default_agent()